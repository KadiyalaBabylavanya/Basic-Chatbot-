{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import bs4 as bs #read and extract information from web pages. beatiful soap\n",
    "import urllib.request #open websites and get the webpage content\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_link=urllib.request.urlopen(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
    "get_link=get_link.read()\n",
    "\n",
    "data=bs.BeautifulSoup(get_link,'lxml')\n",
    " #lxml = read and understand HTML or XML code\n",
    "data_para=data.find_all('p')\n",
    "\n",
    "data_text=''\n",
    "\n",
    "for para in data_para:\n",
    "    data_text+=para.text\n",
    "\n",
    "data_text=data_text.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text=re.sub(r'\\[[0-9]*\\]',' ',data_text) #removes numbers\n",
    "data_text=re.sub(r'\\s+',' ',data_text) #removes + signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sent=nltk.sent_tokenize(data_text)\n",
    "data_words=nltk.word_tokenize(data_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem.wordnet\n",
    "\n",
    "lemma=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [ lemma.lemmatize(token) for token in tokens]\n",
    "\n",
    "punct_rem= dict((ord(punct),None) for punct in string.punctuation)\n",
    "\n",
    "def get_process_text(document):\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punct_rem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_input=(\"hey\",\"hi\",\"baby\")\n",
    "greeting_res=[\"hello\",\"hi baby\"]\n",
    "\n",
    "def welcome(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_input:\n",
    "            return random.choice(greeting_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def generate_response(user_input):\n",
    "    bot_response=''\n",
    "    data_sent.append(user_input)\n",
    "\n",
    "    word_vectorizer=TfidfVectorizer(tokenizer=get_process_text,stop_words='english')\n",
    "    all_words_vectors=word_vectorizer.fit_transform(data_sent)\n",
    "\n",
    "    vals=cosine_similarity(all_words_vectors[-1],all_words_vectors)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat=vals.flatten()\n",
    "    flat.sort()\n",
    "    vector_matched=flat[-2]\n",
    "\n",
    "    if vector_matched==0 or \"tell me about\" in user_response.lower():\n",
    "        bot_response=bot_response+ \"I could not understand you\"\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response=bot_response+data_sent[idx]\n",
    "        return bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I am from AI Science, You can ask me any questions\n",
      "Buddy:however, many ai applications are not perceived as ai: \"a lot of cutting edge ai has filtered into general applications, often without being called ai because once something becomes useful enough and common enough it's not labeled ai anymore.\"\n",
      "Buddy : hello\n",
      "Buddy : hello\n",
      "Buddy:deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\n",
      "Buddy:I could not understand you\n",
      "Bye Buddy!!!\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Hello I am from AI Science, You can ask me any questions\")\n",
    "\n",
    "while(flag):\n",
    "    user_response=input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response not in [\"bye\",\"exit\"]):\n",
    "        if (user_response==\"tnks\"):\n",
    "            flag=False\n",
    "            print(\"You are welcome!!!\")\n",
    "        else:\n",
    "            if(welcome(user_response)!=None):\n",
    "                print(\"Buddy :\",welcome(user_response))\n",
    "            else:\n",
    "                print(\"Buddy:\",end=\"\")\n",
    "                print(generate_response(user_response))\n",
    "                data_sent.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Bye Buddy!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
